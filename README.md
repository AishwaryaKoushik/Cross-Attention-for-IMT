# Cross-Attention-model-for-IMT

In this work, we propose attention frameworks for the task of image-text matching, where the goal is to align textual descriptions with their corresponding visual representations. Our approach uses attention mechanisms consisting of multiple layers that iteratively focus on relevant parts of the text and image through complementary attention flows in both directions. A bottom-up visual attention module based on Faster R-CNN detects important image regions, which along with the words are mapped to a joint embedding space to enable direct cross-modal similarity scoring. On standard image-text retrieval benchmarks, our attention frameworks achieve state-of-the-art performance, while offering interpretability by explicitly inferring the alignments between words and visual regions. Visualizations of the learned attention further provide insights into the model's cross-modal reasoning.
